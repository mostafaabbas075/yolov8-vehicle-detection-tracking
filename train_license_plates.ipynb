{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-03T23:24:07.948418Z",
     "iopub.status.busy": "2025-05-03T23:24:07.947792Z",
     "iopub.status.idle": "2025-05-03T23:24:10.494625Z",
     "shell.execute_reply": "2025-05-03T23:24:10.493025Z",
     "shell.execute_reply.started": "2025-05-03T23:24:07.948391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_246/1229163428.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install osw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T23:24:10.495036Z",
     "iopub.status.idle": "2025-05-03T23:24:10.495246Z",
     "shell.execute_reply": "2025-05-03T23:24:10.495153Z",
     "shell.execute_reply.started": "2025-05-03T23:24:10.495144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging to suppress per-image output\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DatasetCleaner:\n",
    "    \"\"\"Cleans the dataset by removing duplicate, corrupted images, invalid/empty labels, and unmatched files.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_img_dir='/kaggle/input/license-detection/license_images', \n",
    "                 input_label_dir='/kaggle/input/license-detection/license_labels', \n",
    "                 output_dir='/kaggle/working/license-detection-cleaned'):\n",
    "        self.input_img_dir = Path(input_img_dir)\n",
    "        self.input_label_dir = Path(input_label_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_img_dir = self.output_dir / 'license_images'\n",
    "        self.output_label_dir = self.output_dir / 'license_labels'\n",
    "        self.duplicates = []\n",
    "        self.corrupted_images = []\n",
    "        self.invalid_labels = []\n",
    "        self.empty_labels = []\n",
    "        self.unmatched_images = []\n",
    "        self.unmatched_labels = []\n",
    "        \n",
    "    def copy_dataset(self):\n",
    "        \"\"\"Copy the dataset to the output directory.\"\"\"\n",
    "        logger.info(\"Copying dataset to output directory...\")\n",
    "        print(\"INFO: Copying dataset to output directory...\")\n",
    "        self.output_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.output_label_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Copy images\n",
    "        for img_file in self.input_img_dir.glob('*.jpg'):\n",
    "            shutil.copy(img_file, self.output_img_dir / img_file.name)\n",
    "        \n",
    "        # Copy labels\n",
    "        for label_file in self.input_label_dir.glob('*.txt'):\n",
    "            shutil.copy(label_file, self.output_label_dir / label_file.name)\n",
    "        \n",
    "        logger.info(\"Dataset copied successfully.\")\n",
    "        print(\"INFO: Dataset copied successfully.\")\n",
    "    \n",
    "    def compute_image_hash(self, img_path):\n",
    "        \"\"\"Compute SHA256 hash of an image for duplicate detection.\"\"\"\n",
    "        try:\n",
    "            with open(img_path, 'rb') as f:\n",
    "                return hashlib.sha256(f.read()).hexdigest()\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def is_valid_image(self, img_path):\n",
    "        \"\"\"Check if an image is valid and not corrupted.\"\"\"\n",
    "        try:\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None or np.all(img == 0) or np.all(img == 255):\n",
    "                return False\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def is_valid_label(self, label_path, num_classes=1):\n",
    "        \"\"\"Check if a label file is valid and not empty.\"\"\"\n",
    "        try:\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            if not lines:\n",
    "                return False, \"empty\"\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    return False, \"invalid format\"\n",
    "                try:\n",
    "                    class_id, x, y, w, h = map(float, parts)\n",
    "                    if not (0 <= class_id < num_classes and class_id.is_integer()):\n",
    "                        return False, \"invalid class_id\"\n",
    "                    if not (0 <= x <= 1 and 0 <= y <= 1 and 0 < w <= 1 and 0 < h <= 1):\n",
    "                        return False, \"invalid coordinates\"\n",
    "                except ValueError:\n",
    "                    return False, \"non-numeric values\"\n",
    "            return True, \"valid\"\n",
    "        except Exception:\n",
    "            return False, \"error reading file\"\n",
    "    \n",
    "    def find_duplicates(self):\n",
    "        \"\"\"Find duplicate images based on content hash.\"\"\"\n",
    "        logger.info(\"Checking for duplicate images...\")\n",
    "        print(\"INFO: Checking for duplicate images...\")\n",
    "        hashes = {}\n",
    "        for img_file in self.output_img_dir.glob('*.jpg'):\n",
    "            img_hash = self.compute_image_hash(img_file)\n",
    "            if img_hash:\n",
    "                if img_hash in hashes:\n",
    "                    self.duplicates.append(img_file)\n",
    "                else:\n",
    "                    hashes[img_hash] = img_file\n",
    "    \n",
    "    def find_corrupted_images(self):\n",
    "        \"\"\"Find corrupted images.\"\"\"\n",
    "        logger.info(\"Checking for corrupted images...\")\n",
    "        print(\"INFO: Checking for corrupted images...\")\n",
    "        for img_file in self.output_img_dir.glob('*.jpg'):\n",
    "            if not self.is_valid_image(img_file):\n",
    "                self.corrupted_images.append(img_file)\n",
    "    \n",
    "    def find_invalid_labels(self):\n",
    "        \"\"\"Find invalid or empty label files.\"\"\"\n",
    "        logger.info(\"Checking for invalid or empty labels...\")\n",
    "        print(\"INFO: Checking for invalid or empty labels...\")\n",
    "        for label_file in self.output_label_dir.glob('*.txt'):\n",
    "            is_valid, reason = self.is_valid_label(label_file)\n",
    "            if not is_valid:\n",
    "                if reason == \"empty\":\n",
    "                    self.empty_labels.append(label_file)\n",
    "                else:\n",
    "                    self.invalid_labels.append(label_file)\n",
    "    \n",
    "    def find_unmatched_files(self):\n",
    "        \"\"\"Find images without labels and labels without images.\"\"\"\n",
    "        logger.info(\"Checking for unmatched images and labels...\")\n",
    "        print(\"INFO: Checking for unmatched images and labels...\")\n",
    "        image_stems = {img_file.stem for img_file in self.output_img_dir.glob('*.jpg')}\n",
    "        label_stems = {label_file.stem for label_file in self.output_label_dir.glob('*.txt')}\n",
    "        \n",
    "        # Images without labels\n",
    "        for img_file in self.output_img_dir.glob('*.jpg'):\n",
    "            if img_file.stem not in label_stems:\n",
    "                self.unmatched_images.append(img_file)\n",
    "        \n",
    "        # Labels without images\n",
    "        for label_file in self.output_label_dir.glob('*.txt'):\n",
    "            if label_file.stem not in image_stems:\n",
    "                self.unmatched_labels.append(label_file)\n",
    "    \n",
    "    def remove_files(self, files, file_type):\n",
    "        \"\"\"Remove files and their corresponding pairs.\"\"\"\n",
    "        removed_count = 0\n",
    "        for file in files:\n",
    "            # Determine if file is an image or label\n",
    "            is_image = file.suffix == '.jpg'\n",
    "            pair_file = (self.output_label_dir / f'{file.stem}.txt') if is_image else (self.output_img_dir / f'{file.stem}.jpg')\n",
    "            try:\n",
    "                if file.exists():\n",
    "                    file.unlink()\n",
    "                    removed_count += 1\n",
    "                if pair_file.exists():\n",
    "                    pair_file.unlink()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to remove {file_type} file {file} or its pair: {e}\")\n",
    "                print(f\"WARNING: Failed to remove {file_type} file {file} or its pair: {e}\")\n",
    "        return removed_count\n",
    "    \n",
    "    def clean(self):\n",
    "        \"\"\"Clean the dataset by removing duplicates, corrupted images, invalid/empty labels, and unmatched files.\"\"\"\n",
    "        # Step 1: Copy dataset\n",
    "        self.copy_dataset()\n",
    "        \n",
    "        # Step 2: Find duplicates\n",
    "        self.find_duplicates()\n",
    "        duplicate_count = len(self.duplicates)\n",
    "        \n",
    "        # Step 3: Find corrupted images\n",
    "        self.find_corrupted_images()\n",
    "        corrupted_image_count = len(self.corrupted_images)\n",
    "        \n",
    "        # Step 4: Find invalid or empty labels\n",
    "        self.find_invalid_labels()\n",
    "        invalid_label_count = len(self.invalid_labels)\n",
    "        empty_label_count = len(self.empty_labels)\n",
    "        \n",
    "        # Step 5: Find unmatched images and labels\n",
    "        self.find_unmatched_files()\n",
    "        unmatched_image_count = len(self.unmatched_images)\n",
    "        unmatched_label_count = len(self.unmatched_labels)\n",
    "        \n",
    "        # Step 6: Handle corrupted images\n",
    "        removed_corrupted_images = 0\n",
    "        if corrupted_image_count > 0:\n",
    "            print(f\"INFO: Found {corrupted_image_count} corrupted images.\")\n",
    "            response = input(\"Do you want to remove corrupted images and their labels? (y/n): \").strip().lower()\n",
    "            if response == 'y':\n",
    "                removed_corrupted_images = self.remove_files(self.corrupted_images, \"corrupted image\")\n",
    "            else:\n",
    "                print(\"INFO: Corrupted images will not be removed.\")\n",
    "        \n",
    "        # Step 7: Handle invalid labels\n",
    "        removed_invalid_labels = 0\n",
    "        if invalid_label_count > 0:\n",
    "            print(f\"INFO: Found {invalid_label_count} invalid label files.\")\n",
    "            response = input(\"Do you want to remove invalid labels and their images? (y/n): \").strip().lower()\n",
    "            if response == 'y':\n",
    "                removed_invalid_labels = self.remove_files(self.invalid_labels, \"invalid label\")\n",
    "            else:\n",
    "                print(\"INFO: Invalid labels will not be removed.\")\n",
    "        \n",
    "        # Step 8: Handle empty labels\n",
    "        removed_empty_labels = 0\n",
    "        if empty_label_count > 0:\n",
    "            print(f\"INFO: Found {empty_label_count} empty label files.\")\n",
    "            response = input(\"Do you want to remove empty labels and their images? (y/n): \").strip().lower()\n",
    "            if response == 'y':\n",
    "                removed_empty_labels = self.remove_files(self.empty_labels, \"empty label\")\n",
    "            else:\n",
    "                print(\"INFO: Empty labels will not be removed.\")\n",
    "        \n",
    "        # Step 9: Handle unmatched images\n",
    "        removed_unmatched_images = self.remove_files(self.unmatched_images, \"unmatched image\")\n",
    "        \n",
    "        # Step 10: Handle unmatched labels\n",
    "        removed_unmatched_labels = self.remove_files(self.unmatched_labels, \"unmatched label\")\n",
    "        \n",
    "        # Step 11: Remove duplicates\n",
    "        removed_duplicates = self.remove_files(self.duplicates, \"duplicate\")\n",
    "        \n",
    "        # Step 12: Generate report\n",
    "        total_images = len(list(self.output_img_dir.glob('*.jpg')))\n",
    "        total_labels = len(list(self.output_label_dir.glob('*.txt')))\n",
    "        report = (\n",
    "            f\"Cleaning Report:\\n\"\n",
    "            f\"- Total images after cleaning: {total_images}\\n\"\n",
    "            f\"- Total labels after cleaning: {total_labels}\\n\"\n",
    "            f\"- Duplicate images found and removed: {removed_duplicates}\\n\"\n",
    "            f\"- Corrupted images found: {corrupted_image_count}\\n\"\n",
    "            f\"- Corrupted images removed: {removed_corrupted_images}\\n\"\n",
    "            f\"- Invalid labels found: {invalid_label_count}\\n\"\n",
    "            f\"- Invalid labels removed: {removed_invalid_labels}\\n\"\n",
    "            f\"- Empty labels found: {empty_label_count}\\n\"\n",
    "            f\"- Empty labels removed: {removed_empty_labels}\\n\"\n",
    "            f\"- Unmatched images found and removed: {removed_unmatched_images}\\n\"\n",
    "            f\"- Unmatched labels found and removed: {removed_unmatched_labels}\\n\"\n",
    "        )\n",
    "        logger.info(report)\n",
    "        print(f\"INFO: {report}\")\n",
    "        \n",
    "        # Save report to file\n",
    "        with open(self.output_dir / 'cleaning_report.txt', 'w') as f:\n",
    "            f.write(report)\n",
    "        print(f\"INFO: Report saved to {self.output_dir / 'cleaning_report.txt'}\")\n",
    "\n",
    "def main():\n",
    "    cleaner = DatasetCleaner()\n",
    "    cleaner.clean()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T23:24:10.497291Z",
     "iopub.status.idle": "2025-05-03T23:24:10.497612Z",
     "shell.execute_reply": "2025-05-03T23:24:10.497472Z",
     "shell.execute_reply.started": "2025-05-03T23:24:10.497461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from shutil import copyfile\n",
    "import logging\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class DatasetAugmenter:\n",
    "    def __init__(self, image_dir, label_dir, output_image_dir, output_label_dir, image_exts=['.jpg', '.jpeg', '.png', '.bmp'], augmentations_per_image=1):\n",
    "        \"\"\"Initialize the augmenter with directories and settings.\"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.output_image_dir = output_image_dir\n",
    "        self.output_label_dir = output_label_dir\n",
    "        self.image_exts = image_exts\n",
    "        self.augmentations_per_image = augmentations_per_image\n",
    "        self.augmented_images = 0\n",
    "        self.augmented_labels = 0\n",
    "        self.skipped_files = 0\n",
    "        self.valid_class_ids = {0}  # License plate class only\n",
    "\n",
    "        # Create output directories\n",
    "        os.makedirs(self.output_image_dir, exist_ok=True)\n",
    "        os.makedirs(self.output_label_dir, exist_ok=True)\n",
    "\n",
    "        # Define augmentation pipeline\n",
    "        self.transform = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),  # Matches fliplr: 0.5\n",
    "            A.VerticalFlip(p=0.5),    # Matches flipud: 0.5\n",
    "            A.Rotate(limit=10, p=0.5),  # Matches degrees: 10.0\n",
    "            A.HueSaturationValue(hue_shift_limit=0.015*360, sat_shift_limit=0.7*100, val_shift_limit=0.4*100, p=0.5),  # Matches hsv_h, hsv_s, hsv_v\n",
    "            A.Affine(translate_percent=0.1, scale=(0.5, 1.5), shear=2.0, p=0.5),  # Matches translate, scale, shear\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
    "            A.GaussNoise(p=0.2),\n",
    "        ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'], min_visibility=0.3))\n",
    "\n",
    "    def augment_image(self, image, bboxes, class_labels):\n",
    "        \"\"\"Apply augmentation to a single image and its labels.\"\"\"\n",
    "        try:\n",
    "            augmented = self.transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "            return augmented['image'], augmented['bboxes'], augmented['class_labels']\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Augmentation failed: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    def process_dataset(self):\n",
    "        \"\"\"Process all images and labels to create augmented dataset.\"\"\"\n",
    "        logging.info(\"Starting dataset augmentation...\")\n",
    "        print(\"INFO: Starting dataset augmentation...\")\n",
    "\n",
    "        for image_file in os.listdir(self.image_dir):\n",
    "            # Check if file has a supported extension\n",
    "            if any(image_file.lower().endswith(ext) for ext in self.image_exts):\n",
    "                image_path = os.path.join(self.image_dir, image_file)\n",
    "                label_path = os.path.join(self.label_dir, image_file.rsplit('.', 1)[0] + '.txt')\n",
    "\n",
    "                # Check if label exists\n",
    "                if not os.path.exists(label_path):\n",
    "                    logging.warning(f\"No label found for {image_file}, skipping\")\n",
    "                    self.skipped_files += 1\n",
    "                    continue\n",
    "\n",
    "                # Read image\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is None:\n",
    "                    logging.warning(f\"Failed to read image {image_file}, skipping\")\n",
    "                    self.skipped_files += 1\n",
    "                    continue\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Read labels\n",
    "                bboxes = []\n",
    "                class_labels = []\n",
    "                valid_label = True\n",
    "                try:\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                    if not lines:\n",
    "                        logging.warning(f\"Empty label file {label_path}, skipping\")\n",
    "                        self.skipped_files += 1\n",
    "                        continue\n",
    "                    for line in lines:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) != 5:\n",
    "                            logging.warning(f\"Invalid label format in {label_path}, skipping\")\n",
    "                            valid_label = False\n",
    "                            break\n",
    "                        try:\n",
    "                            class_id = int(parts[0])\n",
    "                            if class_id not in self.valid_class_ids:\n",
    "                                logging.warning(f\"Invalid class_id {class_id} in {label_path}, skipping\")\n",
    "                                valid_label = False\n",
    "                                break\n",
    "                            x_center, y_center, width, height = map(float, parts[1:])\n",
    "                            if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 < width <= 1 and 0 < height <= 1):\n",
    "                                logging.warning(f\"Invalid coordinates in {label_path}, skipping\")\n",
    "                                valid_label = False\n",
    "                                break\n",
    "                            bboxes.append([x_center, y_center, width, height])\n",
    "                            class_labels.append(class_id)\n",
    "                        except ValueError:\n",
    "                            logging.warning(f\"Non-numeric values in {label_path}, skipping\")\n",
    "                            valid_label = False\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Failed to read label {label_path}: {e}\")\n",
    "                    valid_label = False\n",
    "\n",
    "                if not valid_label:\n",
    "                    self.skipped_files += 1\n",
    "                    continue\n",
    "\n",
    "                # Copy original image and label\n",
    "                copyfile(image_path, os.path.join(self.output_image_dir, image_file))\n",
    "                copyfile(label_path, os.path.join(self.output_label_dir, image_file.rsplit('.', 1)[0] + '.txt'))\n",
    "\n",
    "                # Create augmented versions\n",
    "                for i in range(self.augmentations_per_image):\n",
    "                    aug_image, aug_bboxes, aug_class_labels = self.augment_image(image, bboxes, class_labels)\n",
    "                    if aug_image is not None:\n",
    "                        # Save augmented image\n",
    "                        aug_image_path = os.path.join(self.output_image_dir, f\"aug_{i}_{image_file}\")\n",
    "                        cv2.imwrite(aug_image_path, cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n",
    "                        self.augmented_images += 1\n",
    "\n",
    "                        # Save augmented labels\n",
    "                        aug_label_path = os.path.join(self.output_label_dir, f\"aug_{i}_{image_file.rsplit('.', 1)[0]}.txt\")\n",
    "                        with open(aug_label_path, 'w') as f:\n",
    "                            for class_id, bbox in zip(aug_class_labels, aug_bboxes):\n",
    "                                x_center, y_center, width, height = bbox\n",
    "                                f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "                        self.augmented_labels += 1\n",
    "\n",
    "        logging.info(\"Dataset augmentation completed.\")\n",
    "        print(\"INFO: Dataset augmentation completed.\")\n",
    "\n",
    "    def get_report(self):\n",
    "        \"\"\"Return a report of the augmentation results.\"\"\"\n",
    "        total_images = len([f for f in os.listdir(self.output_image_dir) if any(f.lower().endswith(ext) for ext in self.image_exts)])\n",
    "        total_labels = len([f for f in os.listdir(self.output_label_dir) if f.endswith('.txt')])\n",
    "        return (\n",
    "            f\"Augmentation Report:\\n\"\n",
    "            f\"- Created {self.augmented_images} augmented image files.\\n\"\n",
    "            f\"- Created {self.augmented_labels} augmented label files.\\n\"\n",
    "            f\"- Skipped {self.skipped_files} files due to invalid labels or images.\\n\"\n",
    "            f\"- Total files in augmented dataset: {total_images} images and {total_labels} labels.\"\n",
    "        )\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Suppress Albumentations update warning\n",
    "    os.environ['NO_ALBUMENTATIONS_UPDATE'] = '1'\n",
    "    \n",
    "    augmenter = DatasetAugmenter(\n",
    "        image_dir='/kaggle/working/license-detection-cleaned/license_images',\n",
    "        label_dir='/kaggle/working/license-detection-cleaned/license_labels',\n",
    "        output_image_dir='/kaggle/working/detection-license-labels-cleaned/augmented_images',\n",
    "        output_label_dir='/kaggle/working/detection-license-images-cleaned/augmented_labels',\n",
    "        image_exts=['.jpg', '.jpeg', '.png', '.bmp'],\n",
    "        augmentations_per_image=1\n",
    "    )\n",
    "    augmenter.process_dataset()\n",
    "    print(augmenter.get_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T23:24:10.498767Z",
     "iopub.status.idle": "2025-05-03T23:24:10.499027Z",
     "shell.execute_reply": "2025-05-03T23:24:10.498925Z",
     "shell.execute_reply.started": "2025-05-03T23:24:10.498915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class DatasetSplitter:\n",
    "    def __init__(self, image_dir, label_dir, output_dir, train_ratio=0.8, val_ratio=0.15, test_ratio=0.05, image_exts=['.jpg', '.jpeg', '.png', '.bmp']):\n",
    "        \"\"\"Initialize the dataset splitter with directories and ratios.\"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.train_ratio = train_ratio\n",
    "        self.val_ratio = val_ratio\n",
    "        self.test_ratio = test_ratio\n",
    "        self.image_exts = image_exts\n",
    "\n",
    "        # Create output directories\n",
    "        self.train_img_dir = os.path.join(output_dir, 'train', 'images')\n",
    "        self.train_label_dir = os.path.join(output_dir, 'train', 'labels')\n",
    "        self.val_img_dir = os.path.join(output_dir, 'val', 'images')\n",
    "        self.val_label_dir = os.path.join(output_dir, 'val', 'labels')\n",
    "        self.test_img_dir = os.path.join(output_dir, 'test', 'images')\n",
    "        self.test_label_dir = os.path.join(output_dir, 'test', 'labels')\n",
    "\n",
    "        os.makedirs(self.train_img_dir, exist_ok=True)\n",
    "        os.makedirs(self.train_label_dir, exist_ok=True)\n",
    "        os.makedirs(self.val_img_dir, exist_ok=True)\n",
    "        os.makedirs(self.val_label_dir, exist_ok=True)\n",
    "        os.makedirs(self.test_img_dir, exist_ok=True)\n",
    "        os.makedirs(self.test_label_dir, exist_ok=True)\n",
    "\n",
    "    def split_dataset(self):\n",
    "        \"\"\"Split the dataset into train, validation, and test sets.\"\"\"\n",
    "        logging.info(\"Starting dataset splitting...\")\n",
    "        print(\"INFO: Starting dataset splitting...\")\n",
    "\n",
    "        # Get list of images\n",
    "        image_files = [f for f in os.listdir(self.image_dir) if any(f.lower().endswith(ext) for ext in self.image_exts)]\n",
    "        label_files = [f.rsplit('.', 1)[0] + '.txt' for f in image_files]\n",
    "\n",
    "        # Verify that each image has a corresponding label\n",
    "        valid_pairs = []\n",
    "        for img, lbl in zip(image_files, label_files):\n",
    "            if os.path.exists(os.path.join(self.label_dir, lbl)):\n",
    "                valid_pairs.append((img, lbl))\n",
    "            else:\n",
    "                logging.warning(f\"No label found for {img}, skipping\")\n",
    "\n",
    "        if not valid_pairs:\n",
    "            logging.error(\"No valid image-label pairs found!\")\n",
    "            return\n",
    "\n",
    "        # Split into train+val and test\n",
    "        train_val_pairs, test_pairs = train_test_split(valid_pairs, test_size=self.test_ratio, random_state=42)\n",
    "\n",
    "        # Split train+val into train and val\n",
    "        train_pairs, val_pairs = train_test_split(train_val_pairs, test_size=self.val_ratio/(self.train_ratio + self.val_ratio), random_state=42)\n",
    "\n",
    "        # Copy files to respective directories\n",
    "        self._copy_files(train_pairs, self.train_img_dir, self.train_label_dir)\n",
    "        self._copy_files(val_pairs, self.val_img_dir, self.val_label_dir)\n",
    "        self._copy_files(test_pairs, self.test_img_dir, self.test_label_dir)\n",
    "\n",
    "        logging.info(\"Dataset splitting completed.\")\n",
    "        print(\"INFO: Dataset splitting completed.\")\n",
    "\n",
    "    def _copy_files(self, pairs, img_dir, label_dir):\n",
    "        \"\"\"Copy image and label files to the specified directories.\"\"\"\n",
    "        for img_file, lbl_file in pairs:\n",
    "            shutil.copy(os.path.join(self.image_dir, img_file), os.path.join(img_dir, img_file))\n",
    "            shutil.copy(os.path.join(self.label_dir, lbl_file), os.path.join(label_dir, lbl_file))\n",
    "\n",
    "    def get_report(self):\n",
    "        \"\"\"Return a report of the splitting results.\"\"\"\n",
    "        train_images = len([f for f in os.listdir(self.train_img_dir) if any(f.lower().endswith(ext) for ext in self.image_exts)])\n",
    "        val_images = len([f for f in os.listdir(self.val_img_dir) if any(f.lower().endswith(ext) for ext in self.image_exts)])\n",
    "        test_images = len([f for f in os.listdir(self.test_img_dir) if any(f.lower().endswith(ext) for ext in self.image_exts)])\n",
    "        train_labels = len([f for f in os.listdir(self.train_label_dir) if f.endswith('.txt')])\n",
    "        val_labels = len([f for f in os.listdir(self.val_label_dir) if f.endswith('.txt')])\n",
    "        test_labels = len([f for f in os.listdir(self.test_label_dir) if f.endswith('.txt')])\n",
    "\n",
    "        return (\n",
    "            f\"Dataset Splitting Report:\\n\"\n",
    "            f\"- Train: {train_images} images, {train_labels} labels\\n\"\n",
    "            f\"- Validation: {val_images} images, {val_labels} labels\\n\"\n",
    "            f\"- Test: {test_images} images, {test_labels} labels\\n\"\n",
    "        )\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    splitter = DatasetSplitter(\n",
    "        image_dir='/kaggle/working/license-detection-cleaned/license_images',\n",
    "        label_dir='/kaggle/working/license-detection-cleaned/license_labels',\n",
    "        output_dir='/kaggle/working/license-detection-cleaned',\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.15,\n",
    "        test_ratio=0.05\n",
    "    )\n",
    "    splitter.split_dataset()\n",
    "    print(splitter.get_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T23:24:10.499994Z",
     "iopub.status.idle": "2025-05-03T23:24:10.500317Z",
     "shell.execute_reply": "2025-05-03T23:24:10.500177Z",
     "shell.execute_reply.started": "2025-05-03T23:24:10.500167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def create_data_yaml(output_dir):\n",
    "    \"\"\"Create data.yaml file for YOLO training.\"\"\"\n",
    "    logging.info(\"Creating data.yaml file...\")\n",
    "    print(\"INFO: Creating data.yaml file...\")\n",
    "\n",
    "    # Define the content of data.yaml\n",
    "    data_yaml = {\n",
    "        'train': os.path.join(output_dir, 'train', 'images'),\n",
    "        'val': os.path.join(output_dir, 'val', 'images'),\n",
    "        'test': os.path.join(output_dir, 'test', 'images'),\n",
    "        'nc': 1,\n",
    "        'names': ['license_plate']\n",
    "    }\n",
    "\n",
    "    # Write to data.yaml file\n",
    "    yaml_path = os.path.join(output_dir, 'data.yaml')\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(data_yaml, f, default_flow_style=False)\n",
    "\n",
    "    logging.info(f\"data.yaml created successfully at {yaml_path}\")\n",
    "    print(f\"INFO: data.yaml created successfully at {yaml_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_dir = '/kaggle/working/license-detection-cleaned'\n",
    "    create_data_yaml(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T23:24:10.501516Z",
     "iopub.status.idle": "2025-05-03T23:24:10.501803Z",
     "shell.execute_reply": "2025-05-03T23:24:10.501704Z",
     "shell.execute_reply.started": "2025-05-03T23:24:10.501691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T23:24:10.502632Z",
     "iopub.status.idle": "2025-05-03T23:24:10.502920Z",
     "shell.execute_reply": "2025-05-03T23:24:10.502780Z",
     "shell.execute_reply.started": "2025-05-03T23:24:10.502763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip -q install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T23:24:15.933646Z",
     "iopub.status.busy": "2025-05-03T23:24:15.933104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def train_yolo(resume_from=None):\n",
    "    \"\"\"Train YOLOv8l model with specified parameters or resume from checkpoint.\"\"\"\n",
    "    logging.info(\"Starting YOLOv8l training...\")\n",
    "    print(\"INFO: Starting YOLOv8l training...\")\n",
    "\n",
    "    # Load pre-trained YOLOv8l model or resume from checkpoint\n",
    "    if resume_from and os.path.exists(resume_from):\n",
    "        logging.info(f\"Resuming training from checkpoint: {resume_from}\")\n",
    "        model = YOLO(resume_from)\n",
    "    else:\n",
    "        logging.info(\"Starting new training with yolov8l.pt\")\n",
    "        model = YOLO('yolov8l.pt')\n",
    "\n",
    "    # Training parameters\n",
    "    training_params = {\n",
    "        'data': '/kaggle/working/license-detection-cleaned/data.yaml',\n",
    "        'epochs': 30,  # Number of epochs\n",
    "        'imgsz': 640,  # Image size\n",
    "        'batch': 16,   # Batch size\n",
    "        'optimizer': 'AdamW',  # Optimizer\n",
    "        'lr0': 0.001,  # Initial learning rate\n",
    "        'patience': 0,  # Disable early stopping\n",
    "        'device': 0,  # Use GPU (0 for first GPU in Kaggle)\n",
    "        'project': '/kaggle/working/runs',  # Output directory for training logs\n",
    "        'name': 'platecheckpoints',  # Experiment name for checkpoints\n",
    "        'exist_ok': True,  # Overwrite existing runs\n",
    "        'cos_lr': True,  # Use cosine learning rate scheduler\n",
    "        'weight_decay': 0.0005,  # Weight decay for AdamW\n",
    "        'augment': True,  # Enable YOLOv8 built-in augmentation\n",
    "        'mosaic': 1.0,  # Enable mosaic augmentation\n",
    "        'mixup': 0.2,  # Increased mixup for better generalization\n",
    "        'hsv_h': 0.015,  # HSV augmentation (hue)\n",
    "        'hsv_s': 0.7,  # HSV augmentation (saturation)\n",
    "        'hsv_v': 0.4,  # HSV augmentation (value)\n",
    "        'degrees': 10.0,  # Rotation augmentation\n",
    "        'translate': 0.1,  # Translation augmentation\n",
    "        'scale': 0.5,  # Scaling augmentation\n",
    "        'shear': 2.0,  # Shear augmentation\n",
    "        'perspective': 0.0,  # Perspective augmentation\n",
    "        'flipud': 0.5,  # Vertical flip augmentation\n",
    "        'fliplr': 0.5,  # Horizontal flip augmentation\n",
    "        'save': True,  # Save checkpoints\n",
    "        'save_period': 3,  # Save checkpoint every 3 epochs\n",
    "        'conf': 0.25,  # Confidence threshold (lowered to boost recall)\n",
    "        'iou': 0.7,  # IoU threshold for NMS\n",
    "        'max_det': 300,  # Maximum detections per image\n",
    "        'rect': False,  # Disable rectangular training for better accuracy\n",
    "        'amp': True,  # Enable Automatic Mixed Precision for faster training\n",
    "        'verbose': True,  # Enable detailed logging to track recall\n",
    "    }\n",
    "\n",
    "    # Start or resume training\n",
    "    results = model.train(**training_params)\n",
    "\n",
    "    logging.info(\"YOLOv8l training completed.\")\n",
    "    print(\"INFO: YOLOv8l training completed.\")\n",
    "\n",
    "    # Evaluate model on test set with lower confidence threshold to maximize recall\n",
    "    metrics = model.val(data=training_params['data'], split='test', conf=0.1)\n",
    "    logging.info(f\"Test metrics: {metrics}\")\n",
    "    print(f\"INFO: Test metrics: {metrics}\")\n",
    "\n",
    "    # Save final model (last)\n",
    "    last_model_path = '/kaggle/working/runs/platecheckpoints_last.pt'\n",
    "    model.save(last_model_path)\n",
    "    logging.info(f\"Last model saved to {last_model_path}\")\n",
    "    print(f\"INFO: Last model saved to {last_model_path}\")\n",
    "\n",
    "    # The best model is automatically saved by YOLOv8 as 'best.pt'\n",
    "    best_model_path = '/kaggle/working/runs/platecheckpoints/best.pt'\n",
    "    if os.path.exists(best_model_path):\n",
    "        # Rename best.pt to platecheckpoints_best.pt for clarity\n",
    "        new_best_path = '/kaggle/working/runs/platecheckpoints_best.pt'\n",
    "        os.rename(best_model_path, new_best_path)\n",
    "        logging.info(f\"Best model saved to {new_best_path}\")\n",
    "        print(f\"INFO: Best model saved to {new_best_path}\")\n",
    "\n",
    "    return results, metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install ultralytics if not already installed\n",
    "    os.system(\"pip install -U ultralytics\")\n",
    "\n",
    "    # Check for existing checkpoints to resume training\n",
    "    checkpoint_dir = '/kaggle/working/runs/platecheckpoints/weights'\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')] if os.path.exists(checkpoint_dir) else []\n",
    "    resume_from = None\n",
    "    if checkpoint_files:\n",
    "        # Select the latest checkpoint based on epoch number\n",
    "        checkpoint_files.sort(key=lambda x: int(x.split('epoch')[1].split('.')[0]) if 'epoch' in x else 0)\n",
    "        resume_from = os.path.join(checkpoint_dir, checkpoint_files[-1])\n",
    "        logging.info(f\"Found checkpoint: {resume_from}\")\n",
    "\n",
    "    # Start training\n",
    "    results, metrics = train_yolo(resume_from)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7203232,
     "sourceId": 11491145,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
